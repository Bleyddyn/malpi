{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import random_split\n",
    "# conflicts with fastai: DataLoader\n",
    "        \n",
    "# Note - you must have torchvision installed for this example\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "        \n",
    "from PIL import Image\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from fastai.vision.all import *\n",
    "\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple, Dict\n",
    "Tensor = TypeVar('torch.tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71400288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results( originals, reconstructed, samples ):\n",
    "    n = len(originals)\n",
    "\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # display original\n",
    "        ax = plt.subplot(3, n, i+1)\n",
    "        plt.imshow(np.transpose(originals[i], (1,2,0)) )\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(3, n, i + n+1)\n",
    "        plt.imshow(np.transpose(reconstructed[i], (1,2,0)))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display samples\n",
    "        ax = plt.subplot(3, n, i + n + n+1)\n",
    "        plt.imshow(np.transpose(samples[i], (1,2,0)))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "def evaluate( val_dataloader, model, n, device ):\n",
    "\n",
    "    batch_features, batch_labels = next(iter(val_dataloader))\n",
    "    #Feature batch shape: torch.Size([32, 3, 32, 32])\n",
    "    originals = batch_features[:n,:]\n",
    "    reconstructed = model(originals)\n",
    "    samples = model.sample(n, device)\n",
    "    return originals, reconstructed, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a076b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 latent_dim: int,\n",
    "                 in_channels: int = 3,\n",
    "                 hidden_dims: List = None,\n",
    "                 beta = 4.0,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.kld_weight = beta\n",
    "        self.img_dim = (in_channels, input_size, input_size) # For use by the TensorboardGenerativeModelImageSampler\n",
    "        self.meta = {}\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        self.final_img = int(input_size / (2**len(hidden_dims))) # Reduce input image size by num of conv layers\n",
    "        dense_calc = int(self.final_img * self.final_img * hidden_dims[-1])\n",
    "        # was: hidden_dims[-1]*4\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    #nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(dense_calc, latent_dim)\n",
    "        self.fc_var = nn.Linear(dense_calc, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, dense_calc)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    #nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
    "                                      kernel_size= 3, stride=1, padding= 1),\n",
    "                            nn.Sigmoid())\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        if isinstance(input, tuple) or isinstance(input, list):\n",
    "            input = input[0]\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "# TODO the 512 in view needs to be pulled from the list of hidden laers\n",
    "        result = result.view(-1, 512, self.final_img, self.final_img)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        self.z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(self.z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0][0]\n",
    "        input = args[1]\n",
    "        mu = args[0][2]\n",
    "        log_var = args[0][3]\n",
    "\n",
    "        kld_weight = self.kld_weight\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "        #recons_loss =F.binary_cross_entropy(recons, input)\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + (kld_weight * kld_loss)\n",
    "        return loss\n",
    "\n",
    "    def loss_function_exp(self, target, recons, mu, log_var ) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        \"\"\"\n",
    "\n",
    "        kld_weight = self.kld_weight\n",
    "        recons_loss =F.mse_loss(recons, target)\n",
    "        #recons_loss =F.binary_cross_entropy(recons, target)\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + (kld_weight * kld_loss)\n",
    "        return loss\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a35f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CIFAR10 data\n",
    "class AEDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Convert a dataset intended for categorical output to one that can\n",
    "        be used to train an autoencoder.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, _ = self.dataset[index]\n",
    "\n",
    "        return image, image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./\", batch_size=128):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        #self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        CIFAR10(self.data_dir, train=True, download=True)\n",
    "        CIFAR10(self.data_dir, train=False, download=True)\n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\":\n",
    "            self.mnist_train = AEDataset( CIFAR10(self.data_dir, train=True, transform=self.transform) )\n",
    "            self.mnist_val = AEDataset( CIFAR10(self.data_dir, train=False, transform=self.transform) )\n",
    "            print( f\"Train dataset: {len(self.mnist_train)}\" )\n",
    "            print( f\"Val dataset: {len(self.mnist_val)}\" )\n",
    "        \n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\":\n",
    "            self.mnist_test = CIFAR10(self.data_dir, train=False, transform=self.transform)\n",
    "    \n",
    "        if stage == \"predict\":\n",
    "            self.mnist_predict = CIFAR10(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.mnist_train, batch_size=self.batch_size)\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.mnist_val, batch_size=self.batch_size)\n",
    "            \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.mnist_test, batch_size=self.batch_size)\n",
    "          \n",
    "    def predict_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.mnist_predict, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66499453",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=30\n",
    "lr=4.7e-4\n",
    "z_dim=128\n",
    "beta=0.00001\n",
    "image_size=32\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastai version of CIFAR10\n",
    "path = untar_data(URLs.CIFAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_lit = CIFAR10DataModule(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_lit.prepare_data()\n",
    "cifar10_lit.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8258a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_vae = VanillaVAE(input_size=image_size, latent_dim=z_dim, beta=beta)\n",
    "fastai_data = DataLoaders(cifar10_lit.train_dataloader(),\n",
    "                          cifar10_lit.val_dataloader())\n",
    "#callbacks = [EarlyStoppingCallback(monitor='valid_loss', min_delta=0.0, patience=5)]\n",
    "callbacks = []\n",
    "learn = Learner(fastai_data, fastai_vae, loss_func=fastai_vae.loss_function)\n",
    "#learn.fit_one_cycle(epochs, lr, cbs=callbacks)\n",
    "print( f\"{learn.opt}\" )\n",
    "if learn.opt is not None:\n",
    "    print( f\"{learn.opt.hypers}\" )\n",
    "print( f\"{learn.cbs}\" )\n",
    "learn.show_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd53927",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = fastai_vae\n",
    "device = torch.device(\"cpu\")\n",
    "vae.to(device)\n",
    "o,r,s = evaluate( cifar10_lit.val_dataloader(), vae, 10, device )\n",
    "r = r[0]\n",
    "o = o.detach().numpy()\n",
    "r = r.detach().numpy()\n",
    "s = s.detach().numpy()\n",
    "show_results( o, r, s )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitVAE(pl.LightningModule):\n",
    "    def __init__(self, lr:float=1e-3, image_size: int=128, latent_dim: int=128, beta:float =4.0, notes: str = None):\n",
    "        super().__init__()\n",
    "        self.model = VanillaVAE(input_size=image_size, latent_dim=latent_dim, beta=beta)\n",
    "        self.lr = lr\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_dim = self.model.img_dim\n",
    "        self.printed = False\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        #optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        # For use by TensorboardGenerativeModelImageSampler\n",
    "        return self.model.decode(z)\n",
    "\n",
    "    def _run_one_batch(self, batch, batch_idx):\n",
    "\n",
    "        recons, _, mu, log_var = self.model.forward(batch)\n",
    "\n",
    "        if isinstance(batch, tuple) or isinstance(batch, list):\n",
    "            batch = batch[0]        \n",
    "\n",
    "        try:\n",
    "            loss_vae = self.model.loss_function_exp( batch, recons, mu, log_var )\n",
    "        except RuntimeError as ex:\n",
    "            raise\n",
    "\n",
    "        return recons, loss_vae\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs, train_loss = self._run_one_batch(batch, batch_idx)\n",
    "        self.log('train_loss', train_loss)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs, val_loss = self._run_one_batch(batch, batch_idx)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs, test_loss = self._run_one_batch(batch, batch_idx)\n",
    "        self.log(\"test_loss\", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24637aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_vae = LitVAE(image_size=image_size, lr=lr, beta=beta)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "trainer.fit(lit_vae, cifar10_lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "o,r,s = evaluate( cifar10_lit.val_dataloader(), lit_vae.model, 10, device )\n",
    "r = r[0]\n",
    "o = o.detach().numpy()\n",
    "r = r.detach().numpy()\n",
    "s = s.detach().numpy()\n",
    "show_results( o, r, s )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d420c9b6",
   "metadata": {},
   "source": [
    "|Hparam|Value|Fastai|Lit|\n",
    "|-----|-|------|-----|\n",
    "|Batch Size|128|Good|Bad|\n",
    "|SGD|||\n",
    "|Adam||Good|Good|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66489c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
