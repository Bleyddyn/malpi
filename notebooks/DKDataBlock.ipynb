{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368d2cd0",
   "metadata": {},
   "source": [
    "Demonstrating how to get DonkeyCar Tub files into a PyTorch/fastai DataBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcdb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.data.transforms import ColReader, Normalize, RandomSplitter\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from donkeycar.parts.tub_v2 import Tub\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a2bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from malpi.dk.train import preprocessFileList, get_data, get_learner, get_autoencoder, train_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ccf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_resnet():\n",
    "    learn2 = cnn_learner(dls, resnet18, loss_func=MSELossFlat(), metrics=[rmse], cbs=ActivationStats(with_hist=True))\n",
    "    learn2.fine_tune(5)\n",
    "    \n",
    "    learn2.recorder.plot_loss()\n",
    "    learn2.show_results(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff9437",
   "metadata": {},
   "source": [
    "The below code is modified from: https://github.com/cmasenas/fastai_navigation_training/blob/master/fastai_train.ipynb.\n",
    "\n",
    "TODO: Figure out how to have multiple output heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15886c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_transform(name, inputs, df_all, batch_tfms, item_tfms, epochs, lr):\n",
    "    dls = get_data(inputs, df_all=df_all, batch_tfms=batch_tfms, item_tfms=item_tfms)\n",
    "    callbacks = [CSVLogger(f\"Transform_{name}.csv\", append=True)]\n",
    "    learn = get_learner(dls)\n",
    "    #learn.no_logging() #Try this to block logging when doing many training test runs\n",
    "    learn.fit_one_cycle(epochs, lr, cbs=callbacks)\n",
    "    #learn.recorder.plot_loss()\n",
    "    #learn.show_results(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d97c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multipel times using a list of Transforms, one at a time.\n",
    "# Compare mean/stdev of best validation loss (or rmse?) for each Transform\n",
    "df_all = get_dataframe(\"track1_warehouse.txt\")\n",
    "transforms = [None]\n",
    "transforms.extend( [*aug_transforms(do_flip=False, size=128)] )\n",
    "for tfm in transforms:\n",
    "    name = \"None\" if tfm is None else str(tfm.__class__.__name__)\n",
    "    print( f\"Transform: {name}\" )\n",
    "    for i in range(5):\n",
    "        print( f\"   Run {i+1}\" )\n",
    "        test_one_transform(name, \"track1_warehouse.txt\", df_all, None, 5, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_learner( learn ):\n",
    "    #dls=nav.dataloaders(df, bs=512)\n",
    "    preds, tgt = learn.get_preds(dl=[dls.one_batch()])\n",
    "\n",
    "    plt.title(\"Target vs Predicted Steering\", fontsize=18, y=1.0)\n",
    "    plt.xlabel(\"Target\", fontsize=14, labelpad=15)\n",
    "    plt.ylabel(\"Predicted\", fontsize=14, labelpad=15)\n",
    "    plt.plot(tgt.T[0], preds.T[0],'bo')\n",
    "    plt.plot([-1,1],[-1,1],'r', linewidth = 4)\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Target vs Predicted Throttle\", fontsize=18, y=1.02)\n",
    "    plt.xlabel(\"Target\", fontsize=14, labelpad=15)\n",
    "    plt.ylabel(\"Predicted\", fontsize=14, labelpad=15)\n",
    "    plt.plot(tgt.T[1], preds.T[1],'bo')\n",
    "    plt.plot([0,1],[0,1],'r', linewidth = 4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbcc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = get_dataframe(\"track1_warehouse.txt\")\n",
    "dls = get_data(\"track1_warehouse.txt\", df_all=df_all, batch_tfms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70395591",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(dls)\n",
    "learn.fit_one_cycle(15, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3873683",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export('models/track1_v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33765689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_pyplot_memory():\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "df_all = get_dataframe(\"track1_warehouse.txt\")\n",
    "\n",
    "transforms=[None,\n",
    "            RandomResizedCrop(128,p=1.0,min_scale=0.5,ratio=(0.9,1.1)),\n",
    "            RandomErasing(sh=0.2, max_count=6,p=1.0),\n",
    "            Brightness(max_lighting=0.4, p=1.0),\n",
    "            Contrast(max_lighting=0.4, p=1.0),\n",
    "            Saturation(max_lighting=0.4, p=1.0)]\n",
    "#dls = get_data(None, df_all, item_tfms=item_tfms, batch_tfms=batch_tfms)\n",
    "\n",
    "for tfm in transforms:\n",
    "    name = \"None\" if tfm is None else str(tfm.__class__.__name__)\n",
    "    if name == \"RandomResizedCrop\":\n",
    "        item_tfms = tfm\n",
    "        batch_tfms = None\n",
    "    else:\n",
    "        item_tfms = None\n",
    "        batch_tfms = tfm\n",
    "        \n",
    "    dls = get_data(\"track1_warehouse.txt\",\n",
    "                   df_all=df_all,\n",
    "                   item_tfms=item_tfms, batch_tfms=batch_tfms)\n",
    "\n",
    "    dls.show_batch(unique=True, show=True)\n",
    "    plt.savefig( f'Transform_{name}.png' )\n",
    "#clear_pyplot_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145bf17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, dls = train_autoencoder( \"tracks_all.txt\", 5, 3e-3, name=\"ae_test1\", verbose=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()\n",
    "learn.show_results(figsize=(20,10))\n",
    "#plt.savefig(name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685477bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx += 1\n",
    "im1 = dls.one_batch()[0]\n",
    "im1_out = learn.model.forward(im1)\n",
    "show_image(im1[idx])\n",
    "show_image(im1_out[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.metrics import rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f00623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "\n",
    "from abc import abstractmethod\n",
    "\n",
    "class BaseVAE(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9dfd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaVAE(BaseVAE):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.kld_weight = 0.00025 # TODO calculate based on: #al_img.shape[0]/ self.num_train_imgs\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, 2, 2)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #print( f\"loss_function: {len(args[0])}  {type(args[0][0])} {args[1].shape}\" )\n",
    "        recons = args[0][0]\n",
    "        input = args[1]\n",
    "        mu = args[0][2]\n",
    "        log_var = args[0][3]\n",
    "\n",
    "        kld_weight = self.kld_weight # kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return loss\n",
    "        #return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f6ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=\"track1_warehouse.txt\"\n",
    "item_tfms = [Resize(64,method=\"squish\")]\n",
    "dls = get_data(input_file, item_tfms=item_tfms, verbose=False, autoencoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3011a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VanillaVAE(3, 64)\n",
    "learn = Learner(dls, vae, loss_func=vae.loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c8ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d040e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
