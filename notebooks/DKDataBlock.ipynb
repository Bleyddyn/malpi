{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368d2cd0",
   "metadata": {},
   "source": [
    "Demonstrating how to get DonkeyCar Tub files into a PyTorch/fastai DataBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcdb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.data.transforms import ColReader, Normalize, RandomSplitter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from donkeycar.parts.tub_v2 import Tub\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a2bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from malpi.dk.train import preprocessFileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823dfdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tubs_from_filelist(file_list, verbose=False):\n",
    "    \"\"\" Load all tubs listed in all files in file_list \"\"\"\n",
    "    tub_dirs = preprocessFileList(file_list)\n",
    "    tubs = []\n",
    "    count = 0\n",
    "    root_path = Path(\"data\")\n",
    "    for item in tub_dirs:\n",
    "        if Path(item).is_dir():\n",
    "            try:\n",
    "                t = Tub(str(item),read_only=True)\n",
    "            except FileNotFoundError as ex:\n",
    "                continue\n",
    "            except ValueError as ex:\n",
    "                # In case the catalog file is empty\n",
    "                continue\n",
    "            tubs.append(t)\n",
    "            count += len(t)\n",
    "    if verbose:\n",
    "        print( f\"Loaded {count} records.\" )\n",
    "        \n",
    "    return tubs\n",
    "        \n",
    "def tubs_from_directory(tub_dir, verbose=False):\n",
    "    \"\"\" Load all tubs in the given directory \"\"\"\n",
    "    tubs = []\n",
    "    count = 0\n",
    "    root_path = Path(tub_dir)\n",
    "    for item in root_path.iterdir():\n",
    "        if item.is_dir():\n",
    "            try:\n",
    "                t = Tub(str(item),read_only=True)\n",
    "                count += len(t)\n",
    "            except FileNotFoundError as ex:\n",
    "                continue\n",
    "            except ValueError as ex:\n",
    "                # In case the catalog file is empty\n",
    "                continue\n",
    "            tubs.append(t)\n",
    "    if verbose:\n",
    "        print( f\"Loaded {count} records.\" )\n",
    "    \n",
    "    return tubs\n",
    "        \n",
    "def dataframe_from_tubs(tubs):\n",
    "    dfs = []\n",
    "    for tub in tubs:\n",
    "        df = pd.DataFrame(tub)\n",
    "        name = Path(tub.base_path).name\n",
    "        pref = os.path.join(tub.base_path, Tub.images() ) + \"/\"\n",
    "        df[\"cam/image_array\"] = pref + df[\"cam/image_array\"]\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6012f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(inputs, verbose=False):\n",
    "    tubs = None\n",
    "    \n",
    "    try:\n",
    "        input_path = Path(inputs)\n",
    "        if input_path.is_dir():\n",
    "            tubs = tubs_from_directory(input_path)\n",
    "    except TypeError as ex:\n",
    "        pass\n",
    "    \n",
    "    if tubs is None:\n",
    "        if isinstance(inputs, str):\n",
    "            inputs = [inputs]\n",
    "        tubs = tubs_from_filelist(inputs)\n",
    "    \n",
    "    if tubs is None:\n",
    "        if verbose:\n",
    "            print( f\"No tubs found at {inputs}\")\n",
    "        return None\n",
    "    \n",
    "    df_all = dataframe_from_tubs(tubs)\n",
    "    \n",
    "    if verbose:\n",
    "        df_all.describe()\n",
    "        \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26fbe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(inputs, df_all=None, batch_tfms=None, verbose=False):\n",
    "    \n",
    "    if df_all is None:\n",
    "        df_all = get_dataframe(inputs, verbose)\n",
    "        \n",
    "    # Normalizing is already done for us, probably because it's defined as an ImageBlock\n",
    "    #tfms = [*aug_transforms(do_flip=False, size=128)]  # Add default transformations except for horizontal flip\\n\",\n",
    "    tfms = [Resize(128,method=\"squish\")]\n",
    "# Add to DataBlock: batch_tfms=tfms\"\n",
    "\n",
    "    pascal = DataBlock(blocks=(ImageBlock, RegressionBlock(n_out=2)),\n",
    "                       splitter=RandomSplitter(),\n",
    "                       get_x=ColReader(\"cam/image_array\"),\n",
    "                       get_y=ColReader(['user/angle','user/throttle']),\n",
    "                       item_tfms=tfms,\n",
    "                       batch_tfms=batch_tfms,\n",
    "                       n_inp=1)\n",
    "    \n",
    "    dls = pascal.dataloaders(df_all)\n",
    "    \n",
    "    if verbose:\n",
    "        dls.show_batch()\n",
    "        dls.one_batch()[0].shape\n",
    "        \n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ccf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_resnet():\n",
    "    learn2 = cnn_learner(dls, resnet18, loss_func=MSELossFlat(), metrics=[rmse], cbs=ActivationStats(with_hist=True))\n",
    "    learn2.fine_tune(5)\n",
    "    \n",
    "    learn2.recorder.plot_loss()\n",
    "    learn2.show_results(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff9437",
   "metadata": {},
   "source": [
    "The below code is modified from: https://github.com/cmasenas/fastai_navigation_training/blob/master/fastai_train.ipynb.\n",
    "\n",
    "TODO: Figure out how to have multiple output heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf25594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(dls):\n",
    "    model = torch.nn.Sequential(\n",
    "        ConvLayer(3, 24, stride=2),\n",
    "        ConvLayer(24, 32, stride=2),\n",
    "        ConvLayer(32, 64, stride=2),\n",
    "        ConvLayer(64, 128, stride=2),\n",
    "        ConvLayer(128, 256, stride=2),\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        Flatten(),\n",
    "        nn.Linear(256, 50),\n",
    "        nn.Linear(50, dls.c)\n",
    "        )\n",
    "#print(model)\n",
    "    callbacks=ActivationStats(with_hist=True)\n",
    "    learn = Learner(dls, model,  loss_func = MSELossFlat(), metrics=[rmse], cbs=callbacks)\n",
    "    #valley = learn.lr_find()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15886c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_transform(name, inputs, df_all, tfm, epochs, lr):\n",
    "    dls = get_data(inputs, df_all=df_all, batch_tfms=tfm)\n",
    "    callbacks = [CSVLogger(f\"Transform_{name}.csv\", append=True)]\n",
    "    learn = get_learner(dls)\n",
    "    #learn.no_logging() #Try this to block logging when doing many training test runs\n",
    "    learn.fit_one_cycle(epochs, lr, cbs=callbacks)\n",
    "    #learn.recorder.plot_loss()\n",
    "    #learn.show_results(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d97c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multipel times using a list of Transforms, one at a time.\n",
    "# Compare mean/stdev of best validation loss (or rmse?) for each Transform\n",
    "df_all = get_dataframe(\"track1_warehouse.txt\")\n",
    "transforms = [None]\n",
    "transforms.extend( [*aug_transforms(do_flip=False, size=128)] )\n",
    "for tfm in transforms:\n",
    "    name = \"None\" if tfm is None else str(tfm.__class__.__name__)\n",
    "    print( f\"Transform: {name}\" )\n",
    "    for i in range(5):\n",
    "        print( f\"   Run {i+1}\" )\n",
    "        test_one_transform(name, \"track1_warehouse.txt\", df_all, None, 5, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dls=nav.dataloaders(df, bs=512)\n",
    "preds, tgt = learn.get_preds(dl=[dls.one_batch()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Target vs Predicted Steering\", fontsize=18, y=1.0)\n",
    "plt.xlabel(\"Target\", fontsize=14, labelpad=15)\n",
    "plt.ylabel(\"Predicted\", fontsize=14, labelpad=15)\n",
    "plt.plot(tgt.T[0], preds.T[0],'bo')\n",
    "plt.plot([-1,1],[-1,1],'r', linewidth = 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b576c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Target vs Predicted Throttle\", fontsize=18, y=1.02)\n",
    "plt.xlabel(\"Target\", fontsize=14, labelpad=15)\n",
    "plt.ylabel(\"Predicted\", fontsize=14, labelpad=15)\n",
    "plt.plot(tgt.T[1], preds.T[1],'bo')\n",
    "plt.plot([0,1],[0,1],'r', linewidth = 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbcc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
